{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Donders Sessions Data Visualization - fMRI workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figures play a central role in the communication of research findings from scientific studies. Therefore, it is important that they accurately represent the underlying data. However, visualizations can be misleading and graphical design choices can bias perceptions (e.g. check out [this blogpost from Flowing Data](https://flowingdata.com/2017/02/09/how-to-spot-visualization-lies/)). This workshop aims to promote critical thinking about visualizing scientific data and provides you with tools and advice that will help you make better figures. More specifically, you will:\n",
    "1. evaluate figures from the neuroscience literature;\n",
    "2. see how data visualization complements statistical testing;\n",
    "3. experience how certain graph types are easier to decode than others;\n",
    "4. learn how you may improve visualization of 2D data;\n",
    "5. learn how you may improve visualization of 3D fMRI data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%python \n",
    "from IPython.display import HTML, IFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Evaluating figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A 2012 survey of 1451 figures from 288 studies in top neuroscience journals found that scientific figures are often unclear and incomplete (Allen et al., 2012). This survey categorized each figure as 2D (e.g. bar, scatter, and line graphs) or 3D (e.g. fMRI statistical maps, time-frequency plots) and answered four questions:\n",
    "1. Is the dependent variable labeled?\n",
    "2. Is the scale of the dependent variable indicated? \n",
    "3. Where applicable, is a measure of uncertainty displayed?\n",
    "4. Is the type of uncertainty (e.g. standard error of the mean) defined in the figure or legend?\n",
    "\n",
    "The figure below shows the survey's results: as compared to displays of 2D data, visualizations of 3D data often lack labels and rarely depict measures of uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"950\"\n",
       "            height=\"300\"\n",
       "            src=\"./data/1_Allen_et_al_2012_FigS1A_Survey_results_by_journal.jpeg\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame object at 0x109d37208>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%python\n",
    "IFrame('./data/1_Allen_et_al_2012_FigS1A_Survey_results_by_journal.jpeg',width= 950, height= 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Survey results separated by journal__. (A) Mean proportion of 2D (white) and 3D (dark gray) figures displaying each feature. Error bars denote 95% non-parametric confidence intervals (10,000 resamples). NN = Nature Neuroscience; NE = Neuron; JN = Journal of Neuroscience; FSN = Frontiers in Systems Neuroscience; NI = Neuroimage; HBM = Human Brain Mapping. \n",
    "\n",
    "_Source_: Allen, E. A., Erhardt, E. B., & Calhoun, V. D. (2012). Data Visualization in the Neurosciences: Overcoming the Curse of Dimensionality. Neuron, 74(4), 603–608. https://doi.org/10.1016/j.neuron.2012.05.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"alert alert-info\">\n",
    "<b>EXERCISE 1</b> <br>\n",
    "\n",
    "You will evaluate one or more figures from recent fMRI studies for clarity and completeness. These figures were published in _Journal of Neuroscience_ and _Cerebral Cortex_. You can guide your evaluation by the four questions above and the more comprehensive checklist below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"1050\"\n",
       "            src=\"./data/checklist_scientific_figures.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame object at 0x109d671d0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%python\n",
    "IFrame('./data/checklist_scientific_figures.pdf',width= 800, height= 1050)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Case 1__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"425\"\n",
       "            src=\"./data/1_1_Diederen_et_al_JNeurosci_2017_Fig05.jpg\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame object at 0x109d67278>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%python\n",
    "IFrame(\"./data/1_1_Diederen_et_al_JNeurosci_2017_Fig05.jpg\", width= 800, height= 425)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Figure 5.__ __A__, Bold response to cues signaling reward variability across the three groups. __B__, Increased BOLD responses to cues signaling smaller reward variability (SD5 > SD10 > SD15; nonlinear contrast weighted by SD−1). We used a stringent initial threshold of p < 1e −11 combined with a minimal cluster size of 5 adjacent voxels as the cue event was associated with very strong signal changes. The cluster threshold was p < 0.05 FWE corrected for multiple comparisons. SD, standard deviation.\n",
    "\n",
    "_Source_: Diederen, K. M. J., Ziauddeen, H., Vestergaard, M. D., Spencer, T., Schultz, W., & Fletcher, P. C. (2017). Dopamine Modulates Adaptive Prediction Error Coding in the Human Midbrain and Striatum. The Journal of Neuroscience, 37(7), 1708–1720. https://doi.org/10.1523/JNEUROSCI.1979-16.2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"525\"\n",
       "            src=\"./data/1_1_Dominguez-Borras_et_al_CerebCortex_2017_Fig03.png\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame object at 0x109d67320>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%python\n",
    "IFrame(\"./data/1_1_Dominguez-Borras_et_al_CerebCortex_2017_Fig03.png\", width=800, height=525)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Figure 3.__ Emotion effects on sensory responses. Left: auditory processing (FaceAud > FaceAlone with fearful faces) > (FaceAud > FaceAlone with neutral faces). Bilateral activations in  PAC (upper panel), representing selective increase to tones during fearful faces which habituated over time, together with the plot of time-dependent responses (±SEM, bottom panel) across conditions for the right PAC peak (mean x y z, 52 −6 −4; P < 0.005 for illustration). Negative values reflect greater habituation for the NEU (neutral) compared with the NEG (fearful) trials. Middle: somatosensory processing (FaceTouch > FaceAlone with fearful faces) > (FaceTouch > FaceAlone with neutral faces). Bilateral increases in primary somatosensory cortex (S1) during fearful faces and average activity (±SEM, bottom middle) across conditions for the right S1 peak (mean x y z, 10 −36 72; P < 0.005 for illustration). Right: visual processing. Decreased responses in left calcarine sulcus induced by emotion (FaceVis > FaceAlone with neutral faces) >  (FaceVis > FaceAlone with fearful faces) and average activity (±SEM, bottom right) across conditions for the left calcarine sulcus peak (mean x y z, −16 −100 −6; P < 0.005 for illustration). Aud: FaceAud condition; Touch: FaceTouch condition; Vis: FaceVis condition. The plane coordinates of each slice are indicated in the upper right-hand corner. Bright colors represent significance levels of contrasts, as indicated by the scale bars (T values). BOLD responses are rendered on an average anatomical image from all participants.\n",
    "\n",
    "_Source_: Domínguez-Borràs, J., Rieger, S. W., Corradi-Dell’Acqua, C., Neveu, R., & Vuilleumier, P. (2017). Fear Spreading Across Senses: Visual Emotional Events Alter Cortical Responses to Touch, Audition, and Vision. Cerebral Cortex, 27(1), 68–82. https://doi.org/10.1093/cercor/bhw337"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"525\"\n",
       "            src=\"./data/1_1_Brooks_JNeurosci_2017_Fig02.jpeg\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame object at 0x109d673c8>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%python\n",
    "IFrame(\"./data/1_1_Brooks_JNeurosci_2017_Fig02.jpeg\", width=800, height=525)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Figure 2.__ Creation of probabilistic brainstem atlas. T2-weighted volumetric images acquired from the 20 healthy subjects were normalized (using the DARTEL technique) and segmented (using the VBM8 toolbox) into gray matter, white matter, or CSF. The gray matter probability maps were registered to one another, to create a probabilistic gray matter atlas (see top row). The color bar represents the probability of a given voxel being gray matter. The main areas of interest were the PAG, LC, and RVM, which were identified by thresholding the atlas at p > 0.7 (i.e., >70% chance of being gray matter) and then outlining the structures of interest on the basis of comparison to known anatomical landmarks taken from the Duvernoy brainstem atlas. Sections shown on the right hand side with structures of interest indicated by red circles (Naidich et al., 2009). All slice locations are given in the MNI coordinates.\n",
    "\n",
    "_Source_: Brooks, J. C. W., Davies, W.-E., & Pickering, A. E. (2017). Resolving the Brainstem Contributions to Attentional Analgesia. Journal of Neuroscience, 37(9), 2279–2291. https://doi.org/10.1523/JNEUROSCI.2193-16.2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"425\"\n",
       "            src=\"./data/1_1_Ewbank_et_al_CerebCortex_2017_Fig03.png\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame object at 0x109d67470>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%python\n",
    "IFrame(\"./data/1_1_Ewbank_et_al_CerebCortex_2017_Fig03.png\", width=800, height=425)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Figure 3__ Experiment 1: RS to faces. Mean parameter estimates (±1 SE) for same- and different-identity conditions (across image-size) in (A) right FFA and (C) left FFA, in control and ASC participants. RS (±1 SE) (i.e., different identity–same identity) in (B) right FFA and (D) left FFA, in control and ASC participants. &ast; P < 0.05, &ast;&ast; P < 0.001, &ast;&ast;&ast; P < 0.0001.\n",
    "\n",
    "_Source_: Ewbank, M. P., Pell, P. J., Powell, T. E., Hagen, V. D., H, E. A., Baron-Cohen, S., & Calder, A. J. (2017). Repetition Suppression and Memory for Faces is Reduced in Adults with Autism Spectrum Conditions. Cerebral Cortex, 27(1), 92–103. https://doi.org/10.1093/cercor/bhw373"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Take home message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When designing and reviewing figures, carefully think about the design and your audience. Also, try out your figures on colleagues who are not directly involved in your project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Further reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Allen, E. A., Erhardt, E. B., & Calhoun, V. D. (2012). Data Visualization in the Neurosciences: Overcoming the Curse of Dimensionality. Neuron, 74(4), 603–608. https://doi.org/10.1016/j.neuron.2012.05.001\n",
    "- Allen, E. A., & Erhardt, E. B. (2017). Visualizing Scientific Data. In J. T. Cacioppo, L. G. Tassinary, & G. G. Berntson (Eds.), Handbook of Psychophysiology (4th ed., pp. 679–697). Cambridge: Cambridge University Press. https://doi.org/10.1017/9781107415782.031\n",
    "- Tufte, E. R. (1983). The Visual Display of Quantitative Information (1st edition). Graphics Press."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Why visualize your data anyway?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting the hands-on part, startup MATLAB 2016b (__Start -> All Programs -> MATLAB R2016b -> MATLAB R2016b__) and run the following code to ensure that all paths are set correctly and all relevant directories are defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% Define some directories\n",
    "workshop_dir = '/Users/bramzandbelt/surfdrive/projects/donders_data_visualization_workshop/';\n",
    "% workshop_dir = 'h:\\common\\temporary\\donders_data_visualization_workshop';\n",
    "data_dir     = fullfile(workshop_dir,'data','workshop_fmri');\n",
    "glm_dir      = fullfile(workshop_dir,'data','workshop_fmri','fmri','stat_stop_left_vs_stop_both');\n",
    "roi_dir      = fullfile(data_dir,'fmri','region_of_interest_masks');\n",
    "\n",
    "% Provide access to MATLAB and SPM\n",
    "addpath(genpath('/Users/bramzandbelt/Documents/MATLAB/spm12/'))\n",
    "% addpath('h:\\common\\matlab\\spm12');\n",
    "\n",
    "% Provide access to a few toolboxes we are going to use\n",
    "addpath(genpath(fullfile(workshop_dir,'opt','gramm')))\n",
    "addpath(genpath(fullfile(workshop_dir,'opt','panel-2.12')))\n",
    "addpath(genpath(fullfile(workshop_dir,'opt','slice_display')))\n",
    "\n",
    "% Provide access to the code written for this workshop\n",
    "addpath(genpath(fullfile(workshop_dir,'src','code','workshop_fmri')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to communicating research findings, another purpose of data visualization is to guide data exploration. A common data analysis mistake is to skip data exploration and to immediately perform statistical tests and look of statistical significance. This happens especially when automated software and processing pipelines are in place. However, it is essential to visually inspect your data first, as it will help you to:\n",
    "- understand broad features of the data;\n",
    "- inspect the qualitative features of the data;\n",
    "- discover new or unexpected patterns in the data.\n",
    "\n",
    "In fact, graphics can be more revealing than typical statistical computations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1. What you will do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate the importance of data visualization in exploratory analysis, you will analyze a dataset known as Anscombe's quartet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2. The data: Anscombe's quartet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anscombe's quartet consists of four sets of x-y pairs. As you will see, descriptive statistics and statistical testing appears to suggest that these four sets are extremely similar, if not identical. However, simply plotting the data reveals that they are very different and that some of them violate the assumptions underlying the statistical tests employed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3. The tool: gramm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[gramm](https://github.com/piermorel/gramm) is a MATLAB toolbox for data visualization that allows you to create a wide range of simple and more complex graphs. Gramm is based on [R's ggplot2 library](http://ggplot2.org/), which in turn was inspired by Leland Wilkinson's book [The Grammar of Graphics](https://www.amazon.de/Grammar-Graphics-Statistics-Computing/dp/1441920331/ref=sr_1_1?ie=UTF8&qid=1489520946&sr=8-1&keywords=wilkinson+%22grammar+of+graphics%22)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Anscombe's quartet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1. Anscombe's quartet - impressions from descriptive and inferential statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, load the dataset and compute some descriptive statistics. Run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% Load Anscombe's quartet dataset\n",
    "load(fullfile(data_dir,'anscombe.mat'))\n",
    "\n",
    "% 1. Number of observations (x,y-pairs)\n",
    "fprintf(1, '01. Number of observations (x,y-pairs) \\n Set 1: %.0f \\n Set 2: %.0f \\n Set 3: %.0f \\n Set 4: %.0f \\n', ... \n",
    "        numel(anscombe.x1), ...\n",
    "        numel(anscombe.x2), ...\n",
    "        numel(anscombe.x3), ...\n",
    "        numel(anscombe.x4));\n",
    "\n",
    "% 2. Mean of X's in each set\n",
    "fprintf(1, '02. Mean of X''s in each set \\n Set 1: %.2f \\n Set 2: %.2f \\n Set 3: %.2f \\n Set 4: %.2f \\n', ... \n",
    "        mean([anscombe.x1;anscombe.x2;anscombe.x3;anscombe.x4],2));\n",
    "\n",
    "% 3. Mean of Y's in each set\n",
    "fprintf(1, '03. Mean of Y''s in each set \\n Set 1: %.2f \\n Set 2: %.2f \\n Set 3: %.2f \\n Set 4: %.2f \\n', ... \n",
    "        mean([anscombe.y1;anscombe.y2;anscombe.y3;anscombe.y4],2));\n",
    "\n",
    "% 4. Linear regression coefficients\n",
    "fprintf(1, '04. Linear regression coefficients (intercept, slope) in each set \\n Set 1: %.2f, %.2f \\n Set 2: %.2f, %.2f \\n Set 3: %.2f, %.2f \\n Set 4: %.2f, %.2f \\n', ... \n",
    "        regress(anscombe.y1',[ones(11,1) anscombe.x1']), ...\n",
    "        regress(anscombe.y2',[ones(11,1) anscombe.x2']), ...\n",
    "        regress(anscombe.y3',[ones(11,1) anscombe.x3']), ...\n",
    "        regress(anscombe.y4',[ones(11,1) anscombe.x4']));\n",
    "\n",
    "% 5. Sum of squares\n",
    "fprintf(1, '05. Sum of squares of X''s in each set \\n Set 1: %.2f \\n Set 2: %.2f \\n Set 3: %.2f \\n Set 4: %.2f \\n', ... \n",
    "        sum((anscombe.x1 - mean(anscombe.x1)).^2), ...\n",
    "        sum((anscombe.x2 - mean(anscombe.x2)).^2), ...\n",
    "        sum((anscombe.x3 - mean(anscombe.x3)).^2), ...\n",
    "        sum((anscombe.x4 - mean(anscombe.x4)).^2));\n",
    "\n",
    "% 6. Correlation coefficients\n",
    "fprintf(1, '06. Correlation coefficients in each set \\n Set 1: %.2f \\n Set 2: %.2f \\n Set 3: %.2f \\n Set 4: %.2f \\n', ... \n",
    "        corr(anscombe.x1',anscombe.y1','type','pearson'), ...\n",
    "        corr(anscombe.x2',anscombe.y2','type','pearson'), ...\n",
    "        corr(anscombe.x3',anscombe.y3','type','pearson'), ...\n",
    "        corr(anscombe.x4',anscombe.y4','type','pearson'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see that from the perspective of descriptive statistics, the four sets look highly similar, if not identical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2. Anscombe's quartet - impressions from graphical analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, display the x-y relationships for the very same datasets. Run the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clear g \n",
    "\n",
    "% Scatter of set 1\n",
    "g(1,1)=gramm('x',anscombe.x1,'y',anscombe.y1);\n",
    "g(1,1).set_names('x','x1','y','y1');\n",
    "g(1,1).geom_point();\n",
    "\n",
    "% Scatter of set 2\n",
    "g(1,2)=gramm('x',anscombe.x2,'y',anscombe.y2);\n",
    "g(1,2).set_names('x','x2','y','y2');\n",
    "g(1,2).geom_point();\n",
    "\n",
    "% Scatter of set 3\n",
    "g(2,1)=gramm('x',anscombe.x3,'y',anscombe.y3);\n",
    "g(2,1).set_names('x','x3','y','y3');\n",
    "g(2,1).geom_point();\n",
    "\n",
    "% Scatter of set 4\n",
    "g(2,2)=gramm('x',anscombe.x4,'y',anscombe.y4);\n",
    "g(2,2).set_names('x','x4','y','y4');\n",
    "g(2,2).geom_point();\n",
    "\n",
    "g.draw();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"600\"\n",
       "            src=\"./data/2_1_Anscombe_quartet.jpg\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame object at 0x109d67518>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%python\n",
    "IFrame(\"./data/2_1_Anscombe_quartet.jpg\", width=800, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is immediately clear that four datasets are very different! You might want to know that a somewhat similar quartet exists for ANOVA interaction effects, described by [Wagenmakers](https://dx.doi.org/10.1016/j.cortex.2015.07.031)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Take home message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Always plot your data! Graphs are essential to good statistical analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Further reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Anscombe, F. J. (1973). Graphs in Statistical Analysis. The American Statistician, 27(1), 17. https://doi.org/10.2307/2682899\n",
    "- Wagenmakers, E.-J. (2015). A quartet of interactions. Cortex, 73, 334–335."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Things to consider when designing visualizations: Graphical perception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The very same dataset can be visualized in many ways (for a nice example, see [this blogpost from Flowing Data](https://flowingdata.com/2017/01/24/one-dataset-visualized-25-ways/#jp-carousel-47350)). The questions in the checklist above are useful to consider when designing your scientific figures. In addition, you may want to take the perceptual capacities of your readers into account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1. What you will do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will experience how certain types of visualizations are more efficiently and more accurately decoded than others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2. The data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data you will be displaying is based on [Haemer, 1947](https://doi.org/10.2307/2681528) and [Allen & Erhardt (2017)](https://dx.doi.org/10.1017/9781107415782.031)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3. The tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Native MATLAB visualization tools used, but all code is under the hood in the function ```graphical_perception```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Decoding information from graphs: visual attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data visualizations communicate numbers in terms of visual attributes (e.g. color, shape, size) of geometric objects (e.g. bars, lines, points). Communication is effective only if the reader is able to perceptually decode the information. Information stored in graphs can be decoded through several visual operations, or 'elementary perceptual tasks'. For example, judging the shade of a color, judging the volume or area of an object, or judging the position of a point along a common scale. As it turns out, decoding efficiency and accuracy differs between elementary perceptual tasks.\n",
    "\n",
    "You will experience this now yourself. The following figure plots the same seven values (across rows) using six different visual attributes (across columns). Can you sort the rows by magnitude? \n",
    "\n",
    "Run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graphical_perception('decoding_visual_attributes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"600\"\n",
       "            src=\"./data/3_1_decoding_visual_attributes.jpg\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame object at 0x109d675c0>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%python\n",
    "IFrame(\"./data/3_1_decoding_visual_attributes.jpg\", width=800, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correct ordering is C, G, E, B, F, A, D. You may have experienced that sorting is more efficient and accurate for attributes on the right as compared to attributes on the left. Indeed, experiments by William Cleveland and Robert McGill in the 1980s showed that the above elementary perceptual tasks can be ordered as followed (from most to least accurate):\n",
    "1. Position along a common scale\n",
    "2. Length and angle\n",
    "3. Area\n",
    "4. Volume\n",
    "5. Color saturation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Decoding information from graphs: difference between curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should also take into account perceptual limitations when visually comparing curves. The following figure plots three panels with two curves each. Can you determine how the difference between curves evolves as a function of x (e.g. increase/deccrease linearly or exponentially, or remains constant)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graphical_perception('decoding_differences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"320\"\n",
       "            src=\"./data/3_2_visualizing_differences.jpg\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame object at 0x109d67668>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%python\n",
    "IFrame(\"./data/3_2_visualizing_differences.jpg\", width=800, height=320)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correct answers are: difference remains constants (left panel), difference increases linearly (middle panel), and difference increases exponentially. If you are surprised by the answers, consider the following. To determine the difference between curves, we must judge the vertical distance between them. However, our brains tend to judge the minimum distance between the curves, rather than the vertical distance. The upshot is that if the difference between curves is of interest, then plot this directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Take home message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When designing visualizations, take into account perceptual abilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Further reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cleveland, W. S., Diaconis, P., & Mcgill, R. (1982). Variables on Scatterplots Look More Highly Correlated When the Scales Are Increased. Science, 216(4550), 1138–1141. https://doi.org/10.1126/science.216.4550.1138\n",
    "- Cleveland, W. S., & McGill, R. (1985). Graphical perception and graphical methods for analyzing scientific data. Science, 229(4716), 828–833.\n",
    "- Haemer, K. W. (1947). Hold That Line. A Plea for the Preservation of Chart Scale Rulings. The American Statistician, 1(1), 25. https://doi.org/10.2307/2681528\n",
    "- Heer, J., & Bostock, M. (2010). Crowdsourcing Graphical Perception: Using Mechanical Turk to Assess Visualization Design. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (pp. 203–212). New York, NY, USA: ACM. https://doi.org/10.1145/1753326.1753357\n",
    "- Lewandowsky, S., & Spence, I. (1989). Discriminating Strata in Scatterplots. Journal of the American Statistical Association, 84(407), 682–688. https://doi.org/10.1080/01621459.1989.10478821"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualizing low-dimensional data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While Anscombe’s quartet is a great example to argue for showing all data, the biggest opposition comes in the form of it being too much information. Bar plots, for example, are widely used for their simplicity. They tell a story that’s easy to understand and compare, one may say. But, as you will see, bar plots are not without problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1. What you will do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will visualize a very simple dataset using a bar plot, box plot, and violin plot. As we go along, you will see that the richness of the display increases and your impression about the underlying data may change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2. The data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will work with a mock dataset from Allen et al., (2012). This dataset consists of 50 samples of a continuous response variable collected under three conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.3. The tool: gramm\n",
    "The visualization tool used is [gramm](https://github.com/piermorel/gramm), but all code is under the hood in the script ```simple_plot```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Showing more, hiding less"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1. Bar plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the data first using a __bar plot__. Run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simple_plot('barplot',data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"600\"\n",
       "            src=\"./data/4_2_1_bar_plot.jpg\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame object at 0x109d67a58>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%python\n",
    "IFrame(\"./data/4_2_1_bar_plot.jpg\", width=800, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bar plot displays the sample mean and standard error. Bar plots have the following strengths:\n",
    "- easy to generate and comprehend;\n",
    "- can efficiently contrast a large number of conditions in a small space;\n",
    "- effective for displaying frequencies or proportions.\n",
    "\n",
    "However, bar plots are not without problems. They reveal very little about the distribution of data and can therefore be misleading (for a great example and funny initiative, see [the #barbarplots Kickstarter project](https://www.kickstarter.com/projects/1474588473/barbarplots)). In fact, bar plots are only suitable for visualizing a set of counts or proportions (e.g. [Krzywinski & Altman, 2014](https://dx.doi.org/10.1038/nmeth.2813)).\n",
    "\n",
    "The bar graph appears to suggest that the conditions have a similar effect on the response variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2. Box plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, visualize the same data first using a __box plot__. Run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simple_plot('boxplot',data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"600\"\n",
       "            src=\"./data/4_2_2_box_plot.jpg\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame object at 0x109d679b0>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%python\n",
    "IFrame(\"./data/4_2_2_box_plot.jpg\", width=800, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, box plots provide a richer view of the data: \n",
    "- the line inside the box displays the __median__; \n",
    "- the bottom and top of the box show the __first and third quartiles__;\n",
    "- the whiskers show the __lowest and highest data point still within 1.5 IQR of the lower and higher quartiles__, respectively;\n",
    "- the dots show __outlying values__ (i.e. beyond 1.5 times the interquartile range), if any.\n",
    "\n",
    "Furthermore, notice that this figure now shows the range of the data and that the response values could take both positive and negative values! Such information is just hidden from your reader if you only plot the means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.3. Violin plots with data superimposed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's visualize the same data again with a __violin plot__ with superimposed the actual data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simple_plot('violinplot',data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"600\"\n",
       "            src=\"./data/4_2_3_violin_plot.jpg\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame object at 0x109d67908>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%python\n",
    "IFrame(\"./data/4_2_3_violin_plot.jpg\", width=800, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The violin plot is somewhat similar to a box plot in that it displays the distribution of the data. It goes beyond a box pot, because it also shows the probability density of the data at different values.\n",
    "\n",
    "The violin plot reveals differences in distributions across the three conditions and suggests that assumption of normality (required for parameteric analyses such as ANOVA) is violated under conditions 2 and 3: under condition 2 the distribution is heavy-tailed, whereas under condition 3 it is bimodal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Take home message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show more, hide less; plot as much of the actual data as you can."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Further reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Kampstra, P. (2008). Beanplot: A Boxplot Alternative for Visual Comparison of Distributions. Journal of\n",
    "Statistical Software, 28(Code Snippet 1), 1 - 9. doi: http://dx.doi.org/10.18637/jss.v028.c01\n",
    "- Rousselet, G. A., Foxe, J. J., & Bolam, J. P. (2016). A few simple steps to improve the description of group results in neuroscience. European Journal of Neuroscience. https://doi.org/10.1111/ejn.13400\n",
    "- Yau, Nathan. \"How to Visualize and Compare Distributions.\" FlowingData. Retrieved March 14, 2017, from\n",
    "http://flowingdata.com/2012/05/15/how-to-visualize-and-compare-distributions/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualizing higher-dimensional data: fMRI statistical parametric maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.1. What you will do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last part of this workshop, you will apply the principle of \"show more, hide less\" to fMRI data. First, you will display an fMRI dataset according to a __commonly used design: a thresholded statistical map overlaid on an anatomical image__. You will see that this design meets several of the evaluation criteria, but fails others. Second, you will display the same dataset using a so-called __dual-coding design__, suggested by [Allen et al. 2012](https://dx.doi.org/10.1016/j.neuron.2012.05.001). Finally, we will complement this dual-coding design with __scatter plots of contrast estimates from a set of regions-of-interest__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.2. The tool: Slice Display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will visualize fMRI data using a MATLAB toolbox, called Slice Display. Slice Display allows you to display fMRI data in terms of thresholded statistical or effect size maps as well as dual-coded images. \n",
    "\n",
    "Slice display uses functions and code from SPM and slover, a visualization tool available within SPM. Slice Display is available through [Github](https://github.com/bramzandbelt/slice_display), so you can also try it out it on your own fMRI datasets, too. Slice Display was developed for this workshop and has not been thoroughly tested. If you find any bugs, please report to [bramzandbelt@gmail.com](mailto:bramzandbelt@gmail.com)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.3. The data: whole-brain voxel-wise random effects analysis of response inhibition experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fMRI dataset that you will visualize is from an fMRI experiment on response inhibition that was performed at the DCCN in 2016. The experimental details are not directly relevant for the purposes of the workshop, but a brief description of the task and analysis procedure is provided below.\n",
    "\n",
    "In this experiment, participants performed a stop-signal task in the scanner while BOLD-fMRI data were collected. On the majority of trials, participants made a bimanual response (i.e. a button press with their left and right index or middle fingers) to a visual stimulus. Occasionally, this visual stimulus was followed by another visual stimulus. There were four different secondary stimuli, instructing them either (__1__) to stop their left hand, but continue their right hand response (__stop_left trial__), or (__2__) to stop their right hand, but continue their left hand response (__stop_right trial__), or (__3__) to stop both their left hand and their right hand response (__stop_both trial__), or (__4__) not to stop any response and to continue the trial as planned (__ignore trial__).\n",
    "\n",
    "The fMRI data was analyzed in SPM12 using a fairly common approach: standard preprocessing steps were followed by first-level statistical analysis according to a general linear model, which in turn was followed by a whole-brain voxel-wise random effects analysis at the group level.\n",
    "Here, we will visualize the results from the analysis focusing on the comparison of brain activation between stop_left and stop_both trials.\n",
    "\n",
    "The data are in the directory ```.../data/workshop_fmri/fmri/```, which is organized as follows:\n",
    "\n",
    "```\n",
    "\n",
    ".\n",
    "|-- anatomical_images                       Contains the group mean anatomical image\n",
    "|-- contrast_images                         Contains 1st-level contrast images for relevant comparisons\n",
    "|-- region_of_interest_masks                Contains binary images of predefined ROIs\n",
    "|-- batch_stat_stop_left_vs_stop_both.mat   SPM Batch of the 2nd-level analysis\n",
    "|-- stat_stop_left_vs_stop_both             Contains files from 2nd-level statistical analysis\n",
    "|-- colormaps.mat                           Custom color-maps for displaying fMRI data\n",
    "|-- roi_data.mat                            Mean parameter estimates across subjects for pre-defined ROIs\n",
    "\n",
    "```\n",
    "\n",
    "Let's load the colormaps that are needed for visualizing the fMRI data. Run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% Load color maps\n",
    "colormaps_file  = fullfile(workshop_dir,'data','workshop_fmri','fmri','colormaps.mat');\n",
    "load(colormaps_file);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.1. Starting point: a thresholded statistical map overlaid on an anatomical map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will begin with visualizing the difference in activation between stop_left and stop-both trials using a commonly used design: a thresholded t-map overlaid on an anatomical map. Almost all task-related fMRI studies that perform statistical analysis using the general linear model display the results in this way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The code below\n",
    "\n",
    "- Lines 2-3 initiate the `layers` and `settings` variables that Slice Display needs in order to determine what to display and how. The second input argument of the function `sd_config_layers` specifies the type and number of layers to display.\n",
    "- Lines 6-7 specify the first layer: it points to the file containing the anatomical scan (line 6) and the colormap for visualizing its data (line 7)\n",
    "- Lines 9-12 specify the second layer: it points to the file containing the _t_-map of the contrast stop_left - stop_both (line 9), defines the color map (line 10), provides a label for the color bar (line 11), and points to a file containing a binary image that specifies which voxels reached statistical significance and which did not (line 12), which is used to threshold the _t_-map.\n",
    "- Lines 15 to 18 specify the orientation (line 15) and position (line 16) of the slices to be displayed, as well as some aspects of the figure layout, including the number of slices per row (line 17) and the figure title (line 18)\n",
    "- Line 21 calls the `sd_display` function that visualizes the data.\n",
    "\n",
    "Now, run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "% Initialize empty layers and settings variables\n",
    "layers                      = sd_config_layers('init',{'truecolor','blob'});\n",
    "settings                    = sd_config_settings('init');\n",
    "    \n",
    "% Specify layers\n",
    "layers(1).color.file        = fullfile(spm('Dir'),'canonical','single_subj_T1.nii');\n",
    "layers(1).color.map         = gray(256);\n",
    "\n",
    "layers(2).color.file        = fullfile(glm_dir,'spmT_0001.nii');\n",
    "layers(2).color.map         = CyBuBkRdYl;\n",
    "layers(2).color.label       = 't-value';\n",
    "layers(2).mask.file         = fullfile(glm_dir,'stop_left_vs_stop_both_significant_voxels_FWE_voxel_level.nii');\n",
    "\n",
    "% Specify settings\n",
    "settings.slice.orientation  = 'axial';\n",
    "settings.slice.disp_slices  = -16:8:72;\n",
    "settings.fig_specs.n.slice_column = 4;\n",
    "settings.fig_specs.title    = 'stop_{left} - stop_{both}';\n",
    "\n",
    "% Display the t-map overlaid on the anatomical MRI\n",
    "sd_display(layers,settings);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"600\"\n",
       "            src=\"./data/5_2_thresholded_t_map.jpg\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame object at 0x109e1d668>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%python\n",
    "IFrame(\"./data/5_2_thresholded_t_map.jpg\", width=800, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the __strengths__ of this visualization's design first:\n",
    "\n",
    "- _Axes_\n",
    "    - The colorbar tick labels indicate that the colormap is linear and comes with a label that describes the variable that is mapped (t-value).\n",
    "    - The map has symmetrical end points, which were inferred from the data (maximum value in the t-map)\n",
    "- _Color and colormap_\n",
    "    - The diverging colormap is consistent with the data type, as the t-map contains both positive and negative values.\n",
    "    - The colors in the colormap do not pose problems for colorblind people (upload the image to Coblis to see for yourself).\n",
    "- _Annotation_\n",
    "    - The contrast's directionality can be inferred from the title and the colorbar.\n",
    "    - Orientation and position of slices are labeled.\n",
    "    - No uncommon abbreviations are displayed.\n",
    "\n",
    "Despite its strengths, this design has some __weaknesses__ too:\n",
    "\n",
    "- _Design_\n",
    "    - Data from many voxels are hidden, due to which this design has a very low 'data-ink ratio' (i.e. the amount of ink devoted to displaying the data as compared to the total amount of ink in the visualization).\n",
    "    - It reduces a rich, complex dataset to a dichotomous representation (statistically significant vs. non-significant).\n",
    "- _Uncertainty_\n",
    "    - Uncertainty is not depicted at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 5.2.2. - Dual-coded images show more and hide less\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now look at a different visualization design that was proposed to address some of these weaknesses ([Allen et al. 2012](https://dx.doi.org/10.1016/j.neuron.2012.05.001)). Specifically, Allen et al. proposed a 'dual-coding' approach for visualizing fMRI statistical parametric maps, in which effect sizes (e.g. contrast estimates) and inferential statistics (e.g. _t_-values) are displayed unthresholded and in one overlay (coded by color hue and opacity, respectively)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below produces a dual-code visualization of the same fMRI dataset that you visualized above.\n",
    "\n",
    "- Line 2 specifies that three layers will be displayed: \n",
    "    - a truecolor layer (for the anatomical brain map), \n",
    "    - a dual-coded layer (for the contrast and _t_-maps), and \n",
    "    - a contour layer (for highlighting voxels/clusters that reached statistical significance).\n",
    "- Lines 9-14 specify the dual-code layer: \n",
    "    - color is coded by the contrast map (line 9) using the diverging cyan-blue-grey-red-yellow colormap in the variable `CyBuGyRdYl` (line 10);\n",
    "    - opacity is coded by the corresponding _t_-map (line 12), with t-values equal to 0 being fully transparent and t-values equal to or greater than 5.17 being fully opaque (line 14);\n",
    "    - labels for the dual-code maps are also provided (lines 11 and 14).\n",
    "- Line 16 specifies the third layer: a contour map that outlines the voxels that reached statistical significance, using the same binary image that was used to threshold the _t_-map in the previous step.\n",
    "\n",
    "Now, run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "% Initialize empty layers and settings variables\n",
    "layers                      = sd_config_layers('init',{'truecolor','dual','contour'});\n",
    "settings                    = sd_config_settings('init');\n",
    "    \n",
    "% Specify layers\n",
    "layers(1).color.file        = fullfile(spm('Dir'),'canonical','single_subj_T1.nii');\n",
    "layers(1).color.map         = gray(256);\n",
    "\n",
    "layers(2).color.file        = fullfile(glm_dir,'con_0001.nii');\n",
    "layers(2).color.map         = CyBuGyRdYl;\n",
    "layers(2).color.label       = '\\beta_{Stop_{left}} - \\beta_{Stop_{both}} (a.u.)';\n",
    "layers(2).opacity.file      = fullfile(glm_dir,'spmT_0001.nii');\n",
    "layers(2).opacity.label     = '| t |';\n",
    "layers(2).opacity.range     = [0 5.17];\n",
    "\n",
    "layers(3).color.file        = fullfile(glm_dir,'stop_left_vs_stop_both_significant_voxels_FWE_voxel_level.nii');\n",
    "\n",
    "% Specify settings\n",
    "settings.slice.orientation  = 'axial';\n",
    "settings.slice.disp_slices  = -16:8:72;\n",
    "settings.fig_specs.n.slice_column = 4;\n",
    "settings.fig_specs.title    = 'stop_{left} - stop_{both}';\n",
    "\n",
    "% Display the t-map overlaid on the anatomical MRI\n",
    "sd_display(layers,settings);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"600\"\n",
       "            src=\"./data/5_3_dual_coded_map.jpg\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame object at 0x109d67ba8>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%python\n",
    "IFrame(\"./data/5_3_dual_coded_map.jpg\", width=800, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's evaluate the __strengths__ of this dual-coding visualization of fMRI data by comparing it to the thresholded visualization:\n",
    "\n",
    "- General\n",
    "    - Note that the strengths of the thresholded visualization also apply to the dual-coding approach and that no information is lost. For example, the transparency encoding still enables spatial localization. Also, the contours distinguish statistically significant clusters from areas where activation did not reach statistical significance.\n",
    "    - the design distinguishes activation that appears to be bilateral but that fails to reach statistical significance in one hemisphere (c.f. left and right parietal cortex in slices Z = 40 mm and Z = 48 mm) from activation that appears to be  truly unilateral (c.f. left vs. right primary motor cortex in slice Z = 64 mm).\n",
    "\n",
    "- Design\n",
    "    - more data are shown, less data are hidden: whereas the original design displayed only the 387 voxels that reached statistical signifiance, the dual-coding design shows the data from all 13627 (!) voxels across the 12 panels (although the majority is almost fully transparent);\n",
    "    - the design highlights that statistically significant blobs are actually the peaks of much larger activation clusters (e.g. slice Z = 48 mm);\n",
    "    - the design emphasizes the quality of the data: parameter estimates that are large (i.e. in the blue-cyan or red-yellow range) and consistent (i.e. largely or completely opaque) are confined to the gray matter, not to the white matter and cerebrospinal fluid.\n",
    "\n",
    "- Uncertainty\n",
    "    - the combination of contrast estimate and t-value now provides a measure of uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.3 - Further improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dual-coding design is a major improvement. However, there is still room for improvement.\n",
    "\n",
    "For example, we could:\n",
    "- display the data overlaid on the __group mean anatomical image__ rather than the canonical single-subject T1-weighted image. The mean anatomical image is more representative of the results displayed (i.e. all layers are based on data from the same subjects) and it puts the precision of the overlaid data into perspective.\n",
    "- show the __mask__ of the statistical analysis (i.e. ```mask.nii```) as to indicate which voxels were included in the statistical analysis and which were not.\n",
    "- plot region-of-interest __parameters estimates in individual subjects__ in order to enhance understanding of the underlying data and to address \n",
    "    - color hue does not naturally translate in magnitude, because colors are perceived as categories (e.g. yellow is not naturally perceived as having a greater value than red)\n",
    "\n",
    "Run the following code to see how the first two of these changes make the visualization even richer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "% Initialize empty layers and settings variables\n",
    "layers                      = sd_config_layers('init',{'truecolor','dual','contour','contour','contour'});\n",
    "settings                    = sd_config_settings('init');\n",
    "    \n",
    "% Specify layers\n",
    "layers(1).color.file        = fullfile(data_dir,'fmri','anatomical_images','group_mean_T1.nii');\n",
    "layers(1).color.map         = gray(256);\n",
    "\n",
    "layers(2).color.file        = fullfile(glm_dir,'con_0001.nii');\n",
    "layers(2).color.map         = CyBuGyRdYl;\n",
    "layers(2).color.label       = '\\beta_{Stop_{left}} - \\beta_{Stop_{both}} (a.u.)';\n",
    "layers(2).opacity.file      = fullfile(glm_dir,'spmT_0001.nii');\n",
    "layers(2).opacity.label     = '| t |';\n",
    "layers(2).opacity.range     = [0 5.17];\n",
    "\n",
    "layers(3).color.file        = fullfile(glm_dir,'stop_left_vs_stop_both_significant_voxels_FWE_voxel_level.nii');\n",
    "layers(4).color.file        = fullfile(glm_dir,'mask.nii');\n",
    "layers(4).color.map         = [1 1 1];\n",
    "\n",
    "layers(5).color.file        = fullfile(roi_dir,'all_rois.nii');\n",
    "layers(5).color.hold        = 1;\n",
    "layers(5).color.map         = [0 1 0];\n",
    "\n",
    "% Specify settings\n",
    "settings.slice.orientation  = 'axial';\n",
    "settings.slice.disp_slices  = -16:8:72;\n",
    "settings.fig_specs.n.slice_column = 4;\n",
    "settings.fig_specs.title    = 'stop_{left} - stop_{both}';\n",
    "\n",
    "% Display the t-map overlaid on the anatomical MRI\n",
    "sd_display(layers,settings);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"600\"\n",
       "            src=\"./data/5_4_dual_coded_map_w_mean_T1_mask_ROIs.jpg\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame object at 0x109d67c50>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%python\n",
    "IFrame(\"./data/5_4_dual_coded_map_w_mean_T1_mask_ROIs.jpg\", width=800, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code to see how scatter plots of contrast estimates across participants add value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "% Load the already extracted ROI data from file\n",
    "load(fullfile(data_dir,'fmri','roi_data.mat'));\n",
    "\n",
    "% Initiatlize gramm object:\n",
    "% - x-axis: contrast estimate for stop_left - go\n",
    "% - y-axis: contrast estimate for stop_both - go\n",
    "clear g\n",
    "g = gramm('x',roi_data.con_stop_left_vs_go, ...\n",
    "          'y',roi_data.con_stop_both_vs_go);\n",
    "\n",
    "% Add subplots: hemisphere varies between rows, ROI between columns\n",
    "g.facet_grid(cellstr(roi_data.roiHemi),roi_data.roiFileName);\n",
    "\n",
    "% Add scatter\n",
    "g.geom_point();\n",
    "\n",
    "% Add inset with histogram of the difference between stop_left vs. stop_both \n",
    "g.stat_cornerhist('edges',-5:1:5,'aspect',0.8);\n",
    "\n",
    "% Add isoline (to enable comparison between stop_left and stop_both)\n",
    "% conditions)\n",
    "g.geom_abline('slope',1,'intercept',0,'style','k--');\n",
    "\n",
    "% Add x=0 line (to enable comparison between stop_left and go)\n",
    "g.geom_vline('xintercept',0,'style','k--');\n",
    "\n",
    "% Add y=0 line (to enable comparison between stop_both and go)\n",
    "g.geom_hline('yintercept',0,'style','k--');\n",
    "\n",
    "% Add axis labels\n",
    "g.set_names('x','stop_left - go','y','stop_both - go','column','','row','');\n",
    "\n",
    "% Adjust axes limits to data and make axes square\n",
    "axis_limits     = [-max(abs([roi_data.con_stop_both_vs_go;roi_data.con_stop_left_vs_go])), ...\n",
    "                   max(abs([roi_data.con_stop_both_vs_go;roi_data.con_stop_left_vs_go]))];\n",
    "\n",
    "g.axe_property('DataAspectRatio',[1 1 1],'XLim',axis_limits,'YLim',axis_limits);\n",
    "\n",
    "% Make plot\n",
    "figure;\n",
    "g.draw();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"768\"\n",
       "            src=\"./data/5_4_ROI_data.jpg\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame object at 0x109d67cf8>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%python\n",
    "IFrame(\"./data/5_4_ROI_data.jpg\", width= 1000, height= 768)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a complex figure, so it is useful to walk through it step by step to understand what is displayed and how it adds to the brain maps.\n",
    "\n",
    "Let's look at the overall design first:\n",
    "- The __figure__ shows parameter estimates for stop_left - go (x-axis) and stop_both - go (y-axis) for five brain regions of interest (columns) in two hemispheres (rows). Each point is a participant.\n",
    "- The __histogram__ on the diagnonal shows the distribution of the contrast stop_left - stop_right. If the histogram is centered right from x = 0, then then the parameter estimate for stop_left is _greater_ than that of stop_both; If the histogram is centered left from x = 0, then the parameter estimate for stop_left is _smaller_ than that of stop_both.\n",
    "- The __dashed line__ is an isoline, showing where stop_left and stop_both parameter estimates are equal. If a point is below this line, then the parameter estimate for stop_left is greater than that of stop_both; if a point is above this line, then the parameter estimate for stop_left is _smaller_ than that of stop_both.\n",
    "- The __axes__ are square and that the axis limits are symmetric, so that data can be compared easily between conditions.\n",
    "\n",
    "How does all this add to the brain maps displayed?\n",
    "- Of course, it further highlights uncertainty/variability in parameter estimates across individuals\n",
    "- Note that the parameter estimates for stop_left and stop_both are against go rather than against baseline, so that not only the difference between stop_left and stop_both can be interpreted (through the histogram and/or the position of the scatter cloud with respect to the dashed line), but also the parameter estimates for stop_left and stop_both themselves.\n",
    "- Observe how the pattern displayed by the histogram, which corresponds to the contrast displayed overlaid on the slices, can be very similar between regions, while the pattern shown by scatter can be very different. For instance, the histograms of left M1 and left pre-SMA show that positive contrast estimates dominate in the sample, reflecting that the contrast estimate for stop_left is greater than that for stop_both. Yet, the scatter clouds are very different from each other. The left M1 response is stronger for go than for stop_both (i.e. all points are below Y = 0), but it tends to be stronger for stop_left than go (i.e. most points are right of X = 0). In contrast, the left pre-SMA response tends to be stronger both for stop_left against go and for stop_both against go.\n",
    "\n",
    "Taken together, a rich picture is provided that adds relevant detail to the brain maps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Take home message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "For visualization of fMRI data, dual-coding designs accompanied by scatter plots provide a rich and powerful alternative to standard thresholded statistical maps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Further reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Allen, E. A., Erhardt, E. B., & Calhoun, V. D. (2012). Data Visualization in the Neurosciences: Overcoming the Curse of Dimensionality. Neuron, 74(4), 603–608. https://doi.org/10.1016/j.neuron.2012.05.001\n",
    "- Rousselet, G. A., Foxe, J. J., & Bolam, J. P. (2016). A few simple steps to improve the description of group results in neuroscience. European Journal of Neuroscience. https://doi.org/10.1111/ejn.13400"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Matlab",
   "language": "matlab",
   "name": "matlab"
  },
  "language_info": {
   "codemirror_mode": "octave",
   "file_extension": ".m",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-octave",
   "name": "matlab",
   "version": "0.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
